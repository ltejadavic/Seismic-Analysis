{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "combined_df = pd.read_excel('combined_sismic_data_updated.xlsx')\n",
    "\n",
    "# Feature selection: extracting time-related features\n",
    "combined_df['YEAR'] = combined_df['FECHA'].apply(lambda x: int(x.split('/')[2]))\n",
    "combined_df['MONTH'] = combined_df['FECHA'].apply(lambda x: int(x.split('/')[1]))\n",
    "combined_df['DAY'] = combined_df['FECHA'].apply(lambda x: int(x.split('/')[0]))\n",
    "\n",
    "# Ensure 'HORA' is a string and handle any NaN or invalid values\n",
    "combined_df['HORA'] = combined_df['HORA'].astype(str).apply(lambda x: x if x != 'nan' else '00:00:00')\n",
    "\n",
    "# Extracting hour, minute, and second\n",
    "combined_df['HOUR'] = combined_df['HORA'].apply(lambda x: int(x.split(':')[0]))\n",
    "combined_df['MINUTE'] = combined_df['HORA'].apply(lambda x: int(x.split(':')[1]))\n",
    "combined_df['SECOND'] = combined_df['HORA'].apply(lambda x: int(x.split(':')[2]))\n",
    "\n",
    "# Selected features\n",
    "features = ['YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND', 'LATITUD', 'LONGITUD', 'PROFUNDIDAD', 'MAGNITUD']\n",
    "\n",
    "# Target variables: Latitude, Longitude, and TIME_SINCE_LAST_EVENT (which will be added next)\n",
    "X = combined_df[features]\n",
    "y = combined_df[['LATITUD', 'LONGITUD']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATETIME</th>\n",
       "      <th>TIME_SINCE_LAST_EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960-01-04 13:18:23</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1960-01-13 15:40:34</td>\n",
       "      <td>218.369722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1960-01-15 09:30:24</td>\n",
       "      <td>41.830556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1960-01-17 02:57:58</td>\n",
       "      <td>41.459444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1960-01-23 03:37:32</td>\n",
       "      <td>144.659444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             DATETIME  TIME_SINCE_LAST_EVENT\n",
       "0 1960-01-04 13:18:23               0.000000\n",
       "1 1960-01-13 15:40:34             218.369722\n",
       "2 1960-01-15 09:30:24              41.830556\n",
       "3 1960-01-17 02:57:58              41.459444\n",
       "4 1960-01-23 03:37:32             144.659444"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert 'FECHA' and 'HORA' to a datetime object for calculating time differences\n",
    "combined_df['DATETIME'] = pd.to_datetime(combined_df['FECHA'] + ' ' + combined_df['HORA'].astype(str))\n",
    "\n",
    "# Sort the dataframe by datetime to ensure correct time differences\n",
    "combined_df = combined_df.sort_values(by='DATETIME').reset_index(drop=True)\n",
    "\n",
    "# Calculate the time difference in hours since the last event\n",
    "combined_df['TIME_SINCE_LAST_EVENT'] = combined_df['DATETIME'].diff().dt.total_seconds() / 3600.0\n",
    "\n",
    "# Fill missing values (first event) with 0\n",
    "combined_df['TIME_SINCE_LAST_EVENT'] = combined_df['TIME_SINCE_LAST_EVENT'].fillna(0)\n",
    "\n",
    "# Update the features list to include the new feature\n",
    "features.append('TIME_SINCE_LAST_EVENT')\n",
    "\n",
    "# Update X with the new feature\n",
    "X = combined_df[features]\n",
    "\n",
    "# Splitting the updated data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the first few rows to verify the feature\n",
    "combined_df[['DATETIME', 'TIME_SINCE_LAST_EVENT']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,472</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">23,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m1,472\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m23,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m102\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,574</span> (95.99 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,574\u001b[0m (95.99 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,574</span> (95.99 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m24,574\u001b[0m (95.99 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 16ms/step - loss: 787.5737 - val_loss: 22.5287\n",
      "Epoch 2/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 21.4147 - val_loss: 19.1361\n",
      "Epoch 3/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - loss: 18.5396 - val_loss: 17.6154\n",
      "Epoch 4/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 15ms/step - loss: 17.2883 - val_loss: 16.8468\n",
      "Epoch 5/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 16.2404 - val_loss: 16.0417\n",
      "Epoch 6/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - loss: 15.8384 - val_loss: 15.7205\n",
      "Epoch 7/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - loss: 15.5549 - val_loss: 15.5032\n",
      "Epoch 8/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 15.4873 - val_loss: 16.3086\n",
      "Epoch 9/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 15.4255 - val_loss: 15.2317\n",
      "Epoch 10/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 15ms/step - loss: 15.4191 - val_loss: 15.3235\n",
      "Epoch 11/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 15ms/step - loss: 15.2511 - val_loss: 15.1598\n",
      "Epoch 12/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 15.4317 - val_loss: 15.3180\n",
      "Epoch 13/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 15.3188 - val_loss: 15.1303\n",
      "Epoch 14/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 15.1800 - val_loss: 15.1375\n",
      "Epoch 15/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 15.2483 - val_loss: 15.1219\n",
      "Epoch 16/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 15.2294 - val_loss: 16.3570\n",
      "Epoch 17/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 15.4583 - val_loss: 15.0957\n",
      "Epoch 18/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 15.2320 - val_loss: 15.1074\n",
      "Epoch 19/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 15.1169 - val_loss: 15.1175\n",
      "Epoch 20/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 15.1911 - val_loss: 15.3794\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Reshape the data for CNN-LSTM\n",
    "n_steps = 3  # number of time steps\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_reshaped = np.array([X_train_scaled[i-n_steps:i] for i in range(n_steps, len(X_train_scaled))])\n",
    "y_train_reshaped = y_train[n_steps:]\n",
    "\n",
    "X_test_reshaped = np.array([X_test_scaled[i-n_steps:i] for i in range(n_steps, len(X_test_scaled))])\n",
    "y_test_reshaped = y_test[n_steps:]\n",
    "\n",
    "# Build the CNN-LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# CNN layers\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# LSTM layer\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "\n",
    "# Dense output layer now predicts 2 values (Latitude and Longitude)\n",
    "model.add(Dense(2))  # 2 outputs for Latitude and Longitude\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train_reshaped, epochs=20, validation_data=(X_test_reshaped, y_test_reshaped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted Latitude: -10.874484062194824\n",
      "Predicted Longitude: -76.21995544433594\n",
      "Expected Date and Time for the next event with Magnitude > 5: 2024-09-01 01:05:04\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Example input data for prediction (replace with actual input data)\n",
    "new_input = np.array([X_test_scaled[0:n_steps]])  # Taking the first few steps from the test data as an example\n",
    "\n",
    "# Reshape the input data to match the model's expected input shape\n",
    "new_input_reshaped = new_input.reshape((1, n_steps, n_features))\n",
    "\n",
    "# Predict the next seismic event's latitude and longitude\n",
    "prediction = model.predict(new_input_reshaped)\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Filter for events with magnitude > 8 that occur after the current date\n",
    "future_events = combined_df[(combined_df['MAGNITUD'] > 5) & (combined_df['DATETIME'] > current_date)]\n",
    "\n",
    "# Check if any future events exist\n",
    "if not future_events.empty:\n",
    "    future_high_magnitude_event = future_events.iloc[0]\n",
    "    print(f\"Predicted Latitude: {prediction[0][0]}\")\n",
    "    print(f\"Predicted Longitude: {prediction[0][1]}\")\n",
    "    print(f\"Expected Date and Time for the next event with Magnitude > 5: {future_high_magnitude_event['DATETIME']}\")\n",
    "else:\n",
    "    print(\"No future seismic events with a magnitude greater than 5 found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Expected Date and Time for the next event with Magnitude > 5: 2024-09-01 01:05:04\n",
      "Predicted Magnitude: 5.2\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Predicted Latitude: -10.874484062194824 ± 4.553580053640547 (Certainty: 77.87%)\n",
      "Predicted Longitude: -76.21995544433594 ± 3.1660341966336807 (Certainty: 76.64%)\n",
      "Expected Date and Time for the next event with Magnitude > 5: 2024-09-01 01:05:04\n",
      "Predicted Magnitude: 5.2\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Example input data for prediction (replace with actual input data)\n",
    "new_input = np.array([X_test_scaled[0:n_steps]])  # Taking the first few steps from the test data as an example\n",
    "\n",
    "# Reshape the input data to match the model's expected input shape\n",
    "new_input_reshaped = new_input.reshape((1, n_steps, n_features))\n",
    "\n",
    "# Predict the next seismic event's latitude and longitude\n",
    "prediction = model.predict(new_input_reshaped)\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Filter for events with magnitude > 5 that occur after the current date\n",
    "future_events = combined_df[(combined_df['MAGNITUD'] > 5) & (combined_df['DATETIME'] > current_date)]\n",
    "\n",
    "# Check if any future events exist\n",
    "if not future_events.empty:\n",
    "    future_high_magnitude_event = future_events.iloc[0]\n",
    "    predicted_magnitude = future_high_magnitude_event['MAGNITUD']\n",
    "    print(f\"Expected Date and Time for the next event with Magnitude > 5: {future_high_magnitude_event['DATETIME']}\")\n",
    "    print(f\"Predicted Magnitude: {predicted_magnitude}\")\n",
    "else:\n",
    "    future_high_magnitude_event = None\n",
    "    print(\"No future seismic events with a magnitude greater than 5 found in the dataset.\")\n",
    "\n",
    "# Predict on the test set to calculate MSE\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "# Convert y_test_reshaped to a NumPy array if it isn't already\n",
    "y_test_reshaped = np.array(y_test_reshaped)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for latitude and longitude\n",
    "mse_latitude = mean_squared_error(y_test_reshaped[:, 0], y_pred[:, 0])\n",
    "mse_longitude = mean_squared_error(y_test_reshaped[:, 1], y_pred[:, 1])\n",
    "\n",
    "# Calculate standard deviations as a measure of certainty\n",
    "std_dev_latitude = np.sqrt(mse_latitude)\n",
    "std_dev_longitude = np.sqrt(mse_longitude)\n",
    "\n",
    "# **Updated Certainty Calculation**\n",
    "# Calculate the range of latitude and longitude in the test set\n",
    "lat_range = np.max(y_test_reshaped[:, 0]) - np.min(y_test_reshaped[:, 0])\n",
    "lon_range = np.max(y_test_reshaped[:, 1]) - np.min(y_test_reshaped[:, 1])\n",
    "\n",
    "# Calculate probability as inverse of the error, normalized to a range [0, 1]\n",
    "probability_latitude = max(0, 1 - std_dev_latitude / lat_range)\n",
    "probability_longitude = max(0, 1 - std_dev_longitude / lon_range)\n",
    "\n",
    "# Ensure probabilities do not exceed 100%\n",
    "probability_latitude = min(probability_latitude, 1)\n",
    "probability_longitude = min(probability_longitude, 1)\n",
    "\n",
    "# Display predictions with certainty and probability\n",
    "print(f\"Predicted Latitude: {prediction[0][0]} ± {std_dev_latitude} (Certainty: {probability_latitude*100:.2f}%)\")\n",
    "print(f\"Predicted Longitude: {prediction[0][1]} ± {std_dev_longitude} (Certainty: {probability_longitude*100:.2f}%)\")\n",
    "\n",
    "# Display expected event details only if a future event is found\n",
    "if future_high_magnitude_event is not None:\n",
    "    print(f\"Expected Date and Time for the next event with Magnitude > 5: {future_high_magnitude_event['DATETIME']}\")\n",
    "    print(f\"Predicted Magnitude: {predicted_magnitude}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
