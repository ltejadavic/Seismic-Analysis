{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "combined_df = pd.read_excel('combined_sismic_data_updated.xlsx')\n",
    "\n",
    "# Feature selection: extracting time-related features\n",
    "combined_df['YEAR'] = combined_df['FECHA'].apply(lambda x: int(x.split('/')[2]))\n",
    "combined_df['MONTH'] = combined_df['FECHA'].apply(lambda x: int(x.split('/')[1]))\n",
    "combined_df['DAY'] = combined_df['FECHA'].apply(lambda x: int(x.split('/')[0]))\n",
    "\n",
    "# Ensure 'HORA' is a string and handle any NaN or invalid values\n",
    "combined_df['HORA'] = combined_df['HORA'].astype(str).apply(lambda x: x if x != 'nan' else '00:00:00')\n",
    "\n",
    "# Extracting hour, minute, and second\n",
    "combined_df['HOUR'] = combined_df['HORA'].apply(lambda x: int(x.split(':')[0]))\n",
    "combined_df['MINUTE'] = combined_df['HORA'].apply(lambda x: int(x.split(':')[1]))\n",
    "combined_df['SECOND'] = combined_df['HORA'].apply(lambda x: int(x.split(':')[2]))\n",
    "\n",
    "# Convert 'FECHA' and 'HORA' to a datetime object for calculating time differences\n",
    "combined_df['DATETIME'] = pd.to_datetime(combined_df['FECHA'] + ' ' + combined_df['HORA'].astype(str))\n",
    "\n",
    "# Sort the dataframe by datetime to ensure correct time differences\n",
    "combined_df = combined_df.sort_values(by='DATETIME').reset_index(drop=True)\n",
    "\n",
    "# Calculate the time difference in hours until the next event (instead of time since the last event)\n",
    "combined_df['TIME_UNTIL_NEXT_EVENT'] = combined_df['DATETIME'].diff(-1).abs().dt.total_seconds() / 3600.0\n",
    "\n",
    "# Shift the TIME_UNTIL_NEXT_EVENT to align with the event that just happened\n",
    "combined_df['TIME_UNTIL_NEXT_EVENT'] = combined_df['TIME_UNTIL_NEXT_EVENT'].shift(-1)\n",
    "\n",
    "# Fill missing values (last event) with a large value or remove it\n",
    "combined_df = combined_df.dropna(subset=['TIME_UNTIL_NEXT_EVENT'])\n",
    "\n",
    "# Selected features including the new feature\n",
    "features = ['YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND', 'LATITUD', 'LONGITUD', 'PROFUNDIDAD', 'MAGNITUD']\n",
    "\n",
    "# Target variables: Latitude, Longitude, Time Until Next Event, Magnitude\n",
    "X = combined_df[features]\n",
    "y = combined_df[['LATITUD', 'LONGITUD', 'TIME_UNTIL_NEXT_EVENT', 'MAGNITUD']]\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">23,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">204</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m1,344\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m23,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m204\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,548</span> (95.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,548\u001b[0m (95.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,548</span> (95.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m24,548\u001b[0m (95.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 17ms/step - loss: 849.6828 - val_loss: 399.0614\n",
      "Epoch 2/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - loss: 463.2939 - val_loss: 396.7393\n",
      "Epoch 3/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - loss: 703.0879 - val_loss: 395.6121\n",
      "Epoch 4/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 15ms/step - loss: 461.0047 - val_loss: 396.5733\n",
      "Epoch 5/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 15ms/step - loss: 749.0748 - val_loss: 394.8191\n",
      "Epoch 6/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 393.5349 - val_loss: 395.2480\n",
      "Epoch 7/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - loss: 478.3469 - val_loss: 394.4949\n",
      "Epoch 8/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - loss: 487.8032 - val_loss: 394.6737\n",
      "Epoch 9/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - loss: 580.8690 - val_loss: 394.0541\n",
      "Epoch 10/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - loss: 453.5742 - val_loss: 394.3748\n",
      "Epoch 11/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - loss: 371.9570 - val_loss: 395.1692\n",
      "Epoch 12/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - loss: 430.3451 - val_loss: 394.1621\n",
      "Epoch 13/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 420.2747 - val_loss: 393.8250\n",
      "Epoch 14/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 396.5033 - val_loss: 394.6343\n",
      "Epoch 15/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 496.4578 - val_loss: 393.8842\n",
      "Epoch 16/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 508.7470 - val_loss: 394.3548\n",
      "Epoch 17/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 389.2123 - val_loss: 395.2256\n",
      "Epoch 18/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 393.3420 - val_loss: 394.9119\n",
      "Epoch 19/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 15ms/step - loss: 490.5927 - val_loss: 394.0560\n",
      "Epoch 20/20\n",
      "\u001b[1m895/895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - loss: 473.3777 - val_loss: 394.1125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Reshape the data for CNN-LSTM\n",
    "n_steps = 3  # number of time steps\n",
    "n_features = X_train.shape[1]  # This includes all features\n",
    "\n",
    "# Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_reshaped = np.array([X_train_scaled[i-n_steps:i] for i in range(n_steps, len(X_train_scaled))])\n",
    "y_train_reshaped = y_train[n_steps:]\n",
    "\n",
    "X_test_reshaped = np.array([X_test_scaled[i-n_steps:i] for i in range(n_steps, len(X_test_scaled))])\n",
    "y_test_reshaped = y_test[n_steps:]\n",
    "\n",
    "# Build the CNN-LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# CNN layers\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# LSTM layer\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "\n",
    "# Dense output layer now predicts 4 values (Latitude, Longitude, Time Until Next Event, Magnitude)\n",
    "model.add(Dense(4))  # 4 outputs for Latitude, Longitude, Time Until Next Event, and Magnitude\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train_reshaped, epochs=20, validation_data=(X_test_reshaped, y_test_reshaped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Predicted Latitude: -11.391161918640137\n",
      "Predicted Longitude: -77.7036361694336\n",
      "Predicted Date and Time of the next event: 2024-12-09 10:26:57.933060\n",
      "Predicted Magnitude: 4.835737228393555\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Predicted Latitude: -11.391161918640137 ± 4.5097002605573815 (Certainty: 77.43%)\n",
      "Predicted Longitude: -77.7036361694336 ± 3.3862917397217807 (Certainty: 77.55%)\n",
      "Predicted Date and Time of the next event: 2024-12-09 10:26:57.933060 ± 39.29922264497465 hours (Certainty: 96.41%)\n",
      "Predicted Magnitude: 4.835737228393555 ± 0.465532616802117 (Certainty: 90.69%)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Example input data for prediction (replace with actual input data)\n",
    "new_input = np.array([X_test_scaled[0:n_steps]])  # Taking the first few steps from the test data as an example\n",
    "\n",
    "# Reshape the input data to match the model's expected input shape\n",
    "new_input_reshaped = new_input.reshape((1, n_steps, n_features))\n",
    "\n",
    "# Predict the next seismic event's latitude, longitude, time until next event, and magnitude\n",
    "prediction = model.predict(new_input_reshaped)\n",
    "\n",
    "# Extract the predictions\n",
    "predicted_latitude = prediction[0][0]\n",
    "predicted_longitude = prediction[0][1]\n",
    "predicted_time_until_next_event = prediction[0][2]\n",
    "predicted_magnitude = prediction[0][3]\n",
    "\n",
    "# Get the last event's date and time\n",
    "last_event_datetime = combined_df['DATETIME'].iloc[-1]\n",
    "\n",
    "# Predict the date of the next event\n",
    "predicted_event_datetime = last_event_datetime + pd.to_timedelta(predicted_time_until_next_event, unit='h')\n",
    "\n",
    "# Display the predictions\n",
    "print(f\"Predicted Latitude: {predicted_latitude}\")\n",
    "print(f\"Predicted Longitude: {predicted_longitude}\")\n",
    "print(f\"Predicted Date and Time of the next event: {predicted_event_datetime}\")\n",
    "print(f\"Predicted Magnitude: {predicted_magnitude}\")\n",
    "\n",
    "# Ensure y_test_reshaped and y_pred are NumPy arrays\n",
    "y_test_reshaped = np.array(y_test_reshaped)\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for latitude, longitude, time since last event, and magnitude\n",
    "mse_latitude = mean_squared_error(y_test_reshaped[:, 0], y_pred[:, 0])\n",
    "mse_longitude = mean_squared_error(y_test_reshaped[:, 1], y_pred[:, 1])\n",
    "mse_time = mean_squared_error(y_test_reshaped[:, 2], y_pred[:, 2])\n",
    "mse_magnitude = mean_squared_error(y_test_reshaped[:, 3], y_pred[:, 3])\n",
    "\n",
    "# Calculate standard deviations as a measure of certainty\n",
    "std_dev_latitude = np.sqrt(mse_latitude)\n",
    "std_dev_longitude = np.sqrt(mse_longitude)\n",
    "std_dev_time = np.sqrt(mse_time)\n",
    "std_dev_magnitude = np.sqrt(mse_magnitude)\n",
    "\n",
    "# Updated Certainty Calculation\n",
    "# Calculate the range of each feature in the test set\n",
    "lat_range = np.max(y_test_reshaped[:, 0]) - np.min(y_test_reshaped[:, 0])\n",
    "lon_range = np.max(y_test_reshaped[:, 1]) - np.min(y_test_reshaped[:, 1])\n",
    "time_range = np.max(y_test_reshaped[:, 2]) - np.min(y_test_reshaped[:, 2])\n",
    "mag_range = np.max(y_test_reshaped[:, 3]) - np.min(y_test_reshaped[:, 3])\n",
    "\n",
    "# Calculate probability as inverse of the error, normalized to a range [0, 1]\n",
    "probability_latitude = max(0, 1 - std_dev_latitude / lat_range)\n",
    "probability_longitude = max(0, 1 - std_dev_longitude / lon_range)\n",
    "probability_time = max(0, 1 - std_dev_time / time_range)\n",
    "probability_magnitude = max(0, 1 - std_dev_magnitude / mag_range)\n",
    "\n",
    "# Ensure probabilities do not exceed 100%\n",
    "probability_latitude = min(probability_latitude, 1)\n",
    "probability_longitude = min(probability_longitude, 1)\n",
    "probability_time = min(probability_time, 1)\n",
    "probability_magnitude = min(probability_magnitude, 1)\n",
    "\n",
    "# Display predictions with certainty and probability\n",
    "print(f\"Predicted Latitude: {predicted_latitude} ± {std_dev_latitude} (Certainty: {probability_latitude*100:.2f}%)\")\n",
    "print(f\"Predicted Longitude: {predicted_longitude} ± {std_dev_longitude} (Certainty: {probability_longitude*100:.2f}%)\")\n",
    "print(f\"Predicted Date and Time of the next event: {predicted_event_datetime} ± {std_dev_time} hours (Certainty: {probability_time*100:.2f}%)\")\n",
    "print(f\"Predicted Magnitude: {predicted_magnitude} ± {std_dev_magnitude} (Certainty: {probability_magnitude*100:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
